"""
LLM é…ç½®ç®¡ç†æ¨¡å—
æä¾›äº¤äº’å¼ LLM é…ç½®å‘å¯¼å’Œé…ç½®ç®¡ç†åŠŸèƒ½
"""
import os
import re
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import httpx
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Confirm, IntPrompt, Prompt
from rich.table import Table

console = Console()


class LLMProvider(Enum):
    """LLM æä¾›å•†æšä¸¾"""
    OPENAI = "openai"
    GEMINI = "gemini"
    MOONSHOT = "moonshot"
    QWEN = "qwen"
    GLM = "glm"
    OPENROUTER = "openrouter"
    OLLAMA = "ollama"
    AZURE = "azure"
    BAIDU = "baidu"
    ALIYUN = "aliyun"
    ZHIPU = "zhipu"
    SKIP = "skip"


@dataclass
class LLMModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    id: str
    name: str
    description: str
    price_hint: str
    context_length: int = 4096
    recommended: bool = False


@dataclass
class LLMProviderConfig:
    """LLM æä¾›å•†é…ç½®"""
    key: str
    display_name: str
    emoji: str
    description: str
    requires_api_key: bool = True
    base_url_hint: Optional[str] = None
    models: List[LLMModelInfo] = field(default_factory=list)
    help_text: str = ""


# æä¾›å•†é…ç½®
PROVIDER_CONFIGS: Dict[LLMProvider, LLMProviderConfig] = {
    LLMProvider.OPENAI: LLMProviderConfig(
        key="openai",
        display_name="OpenAI",
        emoji="ğŸŒ",
        description="ç¨³å®šã€é«˜è´¨é‡ã€é€Ÿåº¦å¿«",
        requires_api_key=True,
        base_url_hint="https://api.openai.com/v1",
        models=[
            LLMModelInfo("gpt-4o-mini", "GPT-4o-mini", "æ€§ä»·æ¯”é«˜ï¼Œé€‚åˆæ—¥å¸¸ä½¿ç”¨", "$0.15 / 1M tokens", 128000, True),
            LLMModelInfo("gpt-4o", "GPT-4o", "æœ€å¼ºæ€§èƒ½ï¼Œé€‚åˆé‡è¦å†…å®¹", "$5.00 / 1M tokens", 128000),
            LLMModelInfo("gpt-4-turbo", "GPT-4 Turbo", "å¹³è¡¡æ€§èƒ½ä¸ä»·æ ¼", "$10.00 / 1M tokens", 128000),
            LLMModelInfo("gpt-3.5-turbo", "GPT-3.5 Turbo", "æˆæœ¬æ•æ„Ÿ", "$0.50 / 1M tokens", 16385),
        ],
        help_text="""
ğŸ“– è·å– API Key æ­¥éª¤ï¼š
   1. è®¿é—® https://platform.openai.com/api-keys
   2. ç™»å½•æ‚¨çš„ OpenAI è´¦å·
   3. ç‚¹å‡» "Create new secret key"
   4. å¤åˆ¶ç”Ÿæˆçš„å¯†é’¥
        """
    ),
    LLMProvider.GEMINI: LLMProviderConfig(
        key="gemini",
        display_name="Google Gemini",
        emoji="ğŸ”·",
        description="Google å‡ºå“ï¼Œå¤šæ¨¡æ€èƒ½åŠ›å¼º",
        requires_api_key=True,
        base_url_hint="https://generativelanguage.googleapis.com/v1beta",
        models=[
            LLMModelInfo("gemini-2.0-flash", "Gemini 2.0 Flash", "é€Ÿåº¦å¿«ï¼Œé€‚åˆæ—¥å¸¸ä½¿ç”¨", "å…è´¹/ä½ä»·", 1000000, True),
            LLMModelInfo("gemini-1.5-pro", "Gemini 1.5 Pro", "Google æœ€å¼ºæ¨¡å‹", "$1.25 / 1M tokens", 2000000),
            LLMModelInfo("gemini-1.5-flash", "Gemini 1.5 Flash", "æ€§ä»·æ¯”é«˜", "$0.075 / 1M tokens", 1000000),
        ],
        help_text="""
ğŸ“– è·å– API Key æ­¥éª¤ï¼š
   1. è®¿é—® https://aistudio.google.com/app/apikey
   2. ç™»å½• Google è´¦å·
   3. ç‚¹å‡» "Create API Key"
   4. å¤åˆ¶ç”Ÿæˆçš„å¯†é’¥
        """
    ),
    LLMProvider.MOONSHOT: LLMProviderConfig(
        key="moonshot",
        display_name="Kimi (æœˆä¹‹æš—é¢)",
        emoji="ğŸŒ™",
        description="å›½äº§é•¿æ–‡æœ¬ä¸“å®¶ï¼Œä¸­æ–‡èƒ½åŠ›å¼º",
        requires_api_key=True,
        base_url_hint="https://api.moonshot.cn/v1",
        models=[
            LLMModelInfo("moonshot-v1-8k", "Kimi K1 (8K)", "è½»é‡å¿«é€Ÿ", "æŒ‰é‡è®¡è´¹", 8192),
            LLMModelInfo("moonshot-v1-32k", "Kimi K1 (32K)", "å¹³è¡¡é€‰æ‹©", "æŒ‰é‡è®¡è´¹", 32768),
            LLMModelInfo("moonshot-v1-128k", "Kimi K1 (128K)", "é•¿æ–‡æœ¬ä¸“å®¶", "æŒ‰é‡è®¡è´¹", 128000, True),
        ],
        help_text="""
ğŸ“– è·å– API Key æ­¥éª¤ï¼š
   1. è®¿é—® https://platform.moonshot.cn/
   2. æ³¨å†Œ/ç™»å½• Kimi å¼€æ”¾å¹³å°è´¦å·
   3. åœ¨ "API Key ç®¡ç†" é¡µé¢åˆ›å»º Key
   4. å¤åˆ¶ç”Ÿæˆçš„å¯†é’¥
        """
    ),
    LLMProvider.QWEN: LLMProviderConfig(
        key="qwen",
        display_name="é€šä¹‰åƒé—® (é˜¿é‡Œ)",
        emoji="ğŸ¤–",
        description="ä¸­æ–‡ä¼˜åŒ–ï¼Œé˜¿é‡Œå‡ºå“",
        requires_api_key=True,
        base_url_hint="https://dashscope.aliyuncs.com/compatible-mode/v1",
        models=[
            LLMModelInfo("qwen-max", "Qwen Max", "é˜¿é‡Œæœ€å¼ºæ¨¡å‹", "æŒ‰é‡è®¡è´¹", 32768, True),
            LLMModelInfo("qwen-plus", "Qwen Plus", "å¹³è¡¡æ€§èƒ½ä»·æ ¼", "æŒ‰é‡è®¡è´¹", 131072),
            LLMModelInfo("qwen-turbo", "Qwen Turbo", "é«˜æ€§ä»·æ¯”", "æŒ‰é‡è®¡è´¹", 65536),
        ],
        help_text="""
ğŸ“– è·å– API Key æ­¥éª¤ï¼š
   1. è®¿é—® https://help.aliyun.com/zh/dashscope/
   2. æ³¨å†Œé˜¿é‡Œäº‘è´¦å·
   3. å¼€é€š DashScope æœåŠ¡
   4. åˆ›å»º API Key
        """
    ),
    LLMProvider.GLM: LLMProviderConfig(
        key="glm",
        display_name="æ™ºè°± GLM",
        emoji="ğŸ§ ",
        description="ä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼Œæ™ºè°± AI å‡ºå“",
        requires_api_key=True,
        base_url_hint="https://open.bigmodel.cn/api/paas/v4",
        models=[
            LLMModelInfo("glm-4", "GLM-4", "æ™ºè°±æœ€å¼ºæ¨¡å‹", "æŒ‰é‡è®¡è´¹", 128000, True),
            LLMModelInfo("glm-4-air", "GLM-4 Air", "é«˜æ€§ä»·æ¯”", "æŒ‰é‡è®¡è´¹", 128000),
            LLMModelInfo("glm-4-flash", "GLM-4 Flash", "è½»é‡å¿«é€Ÿ", "æŒ‰é‡è®¡è´¹", 128000),
        ],
        help_text="""
ğŸ“– è·å– API Key æ­¥éª¤ï¼š
   1. è®¿é—® https://open.bigmodel.cn/
   2. æ³¨å†Œæ™ºè°±è´¦å·
   3. åœ¨ "API Keys" é¡µé¢åˆ›å»º Key
   4. å¤åˆ¶ç”Ÿæˆçš„å¯†é’¥
        """
    ),
    LLMProvider.OPENROUTER: LLMProviderConfig(
        key="openrouter",
        display_name="OpenRouter",
        emoji="ğŸ”—",
        description="èšåˆå¤šå‚å•†ã€æ€§ä»·æ¯”é«˜",
        requires_api_key=True,
        base_url_hint="https://openrouter.ai/api/v1",
        models=[
            LLMModelInfo("anthropic/claude-3.5-sonnet", "Claude 3.5 Sonnet", "æ¨èï¼Œæ¨ç†èƒ½åŠ›å¼º", "$3.00 / 1M tokens", 200000, True),
            LLMModelInfo("openai/gpt-4o", "GPT-4o", "OpenAI æœ€å¼ºæ¨¡å‹", "$5.00 / 1M tokens", 128000),
            LLMModelInfo("google/gemini-pro", "Gemini Pro", "Google å¤§æ¨¡å‹", "$0.50 / 1M tokens", 128000),
            LLMModelInfo("moonshot/kimi-k2", "Kimi K2", "å›½äº§é•¿æ–‡æœ¬æ¨¡å‹", "$0.50 / 1M tokens", 200000),
        ],
        help_text="""
ğŸ“– è·å– API Key æ­¥éª¤ï¼š
   1. è®¿é—® https://openrouter.ai/keys
   2. æ³¨å†Œ/ç™»å½• OpenRouter è´¦å·
   3. åˆ›å»ºæ–°çš„ API Key
   4. å¤åˆ¶ç”Ÿæˆçš„å¯†é’¥
        """
    ),
    LLMProvider.OLLAMA: LLMProviderConfig(
        key="ollama",
        display_name="Ollama (æœ¬åœ°éƒ¨ç½²)",
        emoji="ğŸ ",
        description="å…è´¹ã€éšç§å®‰å…¨ã€æ— éœ€ç½‘ç»œ",
        requires_api_key=False,
        base_url_hint="http://localhost:11434",
        models=[
            LLMModelInfo("qwen2.5:14b", "Qwen 2.5 (14B)", "ä¸­æ–‡æ¨èï¼Œé˜¿é‡Œå¼€æº", "å…è´¹", 32768, True),
            LLMModelInfo("llama3.2:8b", "Llama 3.2 (8B)", "è‹±æ–‡æ¨èï¼ŒMetaå¼€æº", "å…è´¹", 128000),
            LLMModelInfo("mistral:7b", "Mistral (7B)", "å¹³è¡¡é€‰æ‹©ï¼Œæ¬§æ´²å¼€æº", "å…è´¹", 32768),
            LLMModelInfo("phi4:14b", "Phi-4 (14B)", "å¾®è½¯å¼€æºï¼Œå°å·§å¼ºå¤§", "å…è´¹", 16384),
        ],
        help_text="""
ğŸ“– Ollama å®‰è£…æ­¥éª¤ï¼š
   1. å®‰è£… Ollama: curl -fsSL https://ollama.com/install.sh | sh
   2. æ‹‰å–æ¨¡å‹: ollama pull qwen2.5:14b
   3. éªŒè¯è¿è¡Œ: ollama run qwen2.5:14b
   4. ç¡®ä¿æœåŠ¡åœ¨ http://localhost:11434 è¿è¡Œ
        """
    ),
    LLMProvider.AZURE: LLMProviderConfig(
        key="azure",
        display_name="Azure OpenAI",
        emoji="â˜ï¸",
        description="ä¼ä¸šçº§ã€SLAä¿éšœ",
        requires_api_key=True,
        base_url_hint="https://your-resource.openai.azure.com",
        models=[
            LLMModelInfo("gpt-4o", "GPT-4o", "Azure æ‰˜ç®¡çš„ GPT-4o", "æŒ‰éƒ¨ç½²è®¡è´¹", 128000, True),
            LLMModelInfo("gpt-4", "GPT-4", "Azure æ‰˜ç®¡çš„ GPT-4", "æŒ‰éƒ¨ç½²è®¡è´¹", 8192),
            LLMModelInfo("gpt-35-turbo", "GPT-3.5 Turbo", "Azure æ‰˜ç®¡çš„ GPT-3.5", "æŒ‰éƒ¨ç½²è®¡è´¹", 16385),
        ],
        help_text="""
ğŸ“– Azure OpenAI é…ç½®æ­¥éª¤ï¼š
   1. è®¿é—® Azure Portal (https://portal.azure.com)
   2. åˆ›å»º Azure OpenAI æœåŠ¡
   3. åœ¨ "Keys and Endpoint" è·å– API Key
   4. è®°å½• Endpoint URL
   5. éƒ¨ç½²æ¨¡å‹å¹¶è®°å½•éƒ¨ç½²åç§°
        """
    ),
    LLMProvider.BAIDU: LLMProviderConfig(
        key="baidu",
        display_name="æ–‡å¿ƒä¸€è¨€ (ç™¾åº¦)",
        emoji="ğŸ‡¨ğŸ‡³",
        description="ä¸­æ–‡ä¼˜åŒ–ã€å›½å†…è®¿é—®å¿«",
        requires_api_key=True,
        models=[
            LLMModelInfo("ernie-bot-4", "ERNIE Bot 4.0", "ç™¾åº¦æœ€å¼ºæ¨¡å‹", "æŒ‰é‡è®¡è´¹", 8192, True),
            LLMModelInfo("ernie-bot", "ERNIE Bot", "ç™¾åº¦æ ‡å‡†æ¨¡å‹", "æŒ‰é‡è®¡è´¹", 8192),
        ],
        help_text="""
ğŸ“– æ–‡å¿ƒä¸€è¨€ API Key è·å–ï¼š
   1. è®¿é—® https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Ilkkrb0i5
   2. æ³¨å†Œç™¾åº¦æ™ºèƒ½äº‘è´¦å·
   3. åˆ›å»ºåº”ç”¨è·å– API Key å’Œ Secret Key
        """
    ),
    LLMProvider.ALIYUN: LLMProviderConfig(
        key="aliyun",
        display_name="é€šä¹‰åƒé—® (æ—§ç‰ˆ)",
        emoji="ğŸ‡¨ğŸ‡³",
        description="å·²è¿ç§»åˆ° Qwen ç‹¬ç«‹é…ç½®ï¼Œè¯·ä½¿ç”¨æ–°ç‰ˆ",
        requires_api_key=True,
        models=[
            LLMModelInfo("qwen-max", "Qwen Max", "é˜¿é‡Œæœ€å¼ºæ¨¡å‹", "æŒ‰é‡è®¡è´¹", 32768, True),
        ],
        help_text="""
âš ï¸ æç¤ºï¼šå»ºè®®ç›´æ¥ä½¿ç”¨ Qwen é…ç½®ï¼Œæ”¯æŒæ›´å¤šæ¨¡å‹é€‰é¡¹
        """
    ),
    LLMProvider.ZHIPU: LLMProviderConfig(
        key="zhipu",
        display_name="æ™ºè°± AI (æ—§ç‰ˆ)",
        emoji="ğŸ‡¨ğŸ‡³",
        description="å·²è¿ç§»åˆ° GLM ç‹¬ç«‹é…ç½®ï¼Œè¯·ä½¿ç”¨æ–°ç‰ˆ",
        requires_api_key=True,
        models=[
            LLMModelInfo("glm-4", "GLM-4", "æ™ºè°±æœ€å¼ºæ¨¡å‹", "æŒ‰é‡è®¡è´¹", 128000, True),
        ],
        help_text="""
âš ï¸ æç¤ºï¼šå»ºè®®ç›´æ¥ä½¿ç”¨ GLM é…ç½®ï¼Œæ”¯æŒæ›´å¤šæ¨¡å‹é€‰é¡¹
        """
    ),
}


@dataclass
class LLMConfig:
    """LLM é…ç½®æ•°æ®ç±»"""
    provider: str = ""
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    model: str = ""
    api_version: Optional[str] = None  # Azure ä¸“ç”¨
    deployment: Optional[str] = None   # Azure ä¸“ç”¨
    secret_key: Optional[str] = None   # ç™¾åº¦ä¸“ç”¨
    
    # åŠŸèƒ½å¼€å…³
    enable_summary: bool = True
    enable_quality_check: bool = True
    enable_tagging: bool = False
    enable_recommendation: bool = False
    
    # æ‘˜è¦è®¾ç½®
    summary_length: str = "medium"  # short/medium/long
    
    def is_configured(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²é…ç½®"""
        return bool(self.provider and self.model)
    
    def get_masked_api_key(self) -> str:
        """è·å–è„±æ•çš„ API Key"""
        if not self.api_key:
            return "æœªè®¾ç½®"
        if len(self.api_key) <= 8:
            return "****"
        return f"{self.api_key[:4]}****{self.api_key[-4:]}"


class LLMConfigManager:
    """LLM é…ç½®ç®¡ç†å™¨"""
    
    ENV_FILE_PATH = Path(__file__).parent.parent / ".env"
    
    def __init__(self):
        self.config = LLMConfig()
        self._load_from_env()
    
    def _load_from_env(self):
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        from src.config import get_settings
        import os
        
        settings = get_settings()
        
        # æ£€æµ‹æä¾›å•†ï¼ˆä¼˜å…ˆæ£€æŸ¥ç‰¹å®šçš„ LLM_PROVIDER å˜é‡ï¼‰
        provider = os.getenv("LLM_PROVIDER", "").lower()
        
        if provider:
            self.config.provider = provider
            self.config.api_key = os.getenv("LLM_API_KEY", "")
            self.config.base_url = os.getenv("LLM_BASE_URL", "")
            self.config.model = os.getenv("LLM_MODEL", "")
        elif settings.openai_api_key:
            # å…¼å®¹æ—§ç‰ˆé…ç½®æ–¹å¼
            if settings.openai_base_url and "openrouter" in settings.openai_base_url:
                self.config.provider = "openrouter"
            elif settings.openai_base_url and "azure" in settings.openai_base_url:
                self.config.provider = "azure"
            elif settings.openai_base_url and "moonshot" in settings.openai_base_url:
                self.config.provider = "moonshot"
            elif settings.openai_base_url and "generativelanguage" in settings.openai_base_url:
                self.config.provider = "gemini"
            elif settings.openai_base_url and "bigmodel" in settings.openai_base_url:
                self.config.provider = "glm"
            elif settings.openai_base_url and "dashscope" in settings.openai_base_url:
                self.config.provider = "qwen"
            else:
                self.config.provider = "openai"
            
            self.config.api_key = settings.openai_api_key
            self.config.base_url = settings.openai_base_url
            self.config.model = settings.openai_model or "gpt-4o-mini"
        
        # åŠ è½½åŠŸèƒ½å¼€å…³é…ç½®ï¼ˆä» Settings è¯»å–ï¼‰
        self.config.enable_summary = settings.enable_summary
        self.config.enable_quality_check = settings.enable_quality_check
        self.config.enable_tagging = settings.enable_tagging
        self.config.enable_recommendation = settings.enable_recommendation
        self.config.summary_length = settings.summary_length
    
    def get_current_config(self) -> LLMConfig:
        """è·å–å½“å‰é…ç½®"""
        return self.config
    
    def save_config(self, config: LLMConfig):
        """ä¿å­˜é…ç½®åˆ° .env æ–‡ä»¶"""
        self.config = config
        
        # è¯»å–ç°æœ‰ .env å†…å®¹
        env_content = {}
        if self.ENV_FILE_PATH.exists():
            with open(self.ENV_FILE_PATH, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#") and "=" in line:
                        key, value = line.split("=", 1)
                        env_content[key] = value
        
        # æ›´æ–° LLM ç›¸å…³é…ç½®
        if config.provider == "skip":
            # åˆ é™¤ LLM é…ç½®
            for key in ["LLM_PROVIDER", "LLM_API_KEY", "LLM_BASE_URL", "LLM_MODEL",
                       "OPENAI_API_KEY", "OPENAI_BASE_URL", "OPENAI_MODEL"]:
                env_content.pop(key, None)
        else:
            # ä½¿ç”¨æ–°çš„é…ç½®æ ¼å¼
            env_content["LLM_PROVIDER"] = config.provider
            env_content["LLM_API_KEY"] = config.api_key or ""
            env_content["LLM_MODEL"] = config.model or ""
            
            if config.base_url:
                env_content["LLM_BASE_URL"] = config.base_url
            elif "LLM_BASE_URL" in env_content:
                del env_content["LLM_BASE_URL"]
            
            # åŒæ—¶ä¿ç•™å…¼å®¹æ—§ç‰ˆçš„é…ç½®
            env_content["OPENAI_API_KEY"] = config.api_key or ""
            env_content["OPENAI_MODEL"] = config.model or ""
            if config.base_url:
                env_content["OPENAI_BASE_URL"] = config.base_url
            elif "OPENAI_BASE_URL" in env_content:
                del env_content["OPENAI_BASE_URL"]
            
            # ä¿å­˜åŠŸèƒ½å¼€å…³é…ç½®
            env_content["ENABLE_SUMMARY"] = str(config.enable_summary).lower()
            env_content["ENABLE_QUALITY_CHECK"] = str(config.enable_quality_check).lower()
            env_content["ENABLE_TAGGING"] = str(config.enable_tagging).lower()
            env_content["ENABLE_RECOMMENDATION"] = str(config.enable_recommendation).lower()
            env_content["SUMMARY_LENGTH"] = config.summary_length
        
        # å†™å…¥æ–‡ä»¶
        with open(self.ENV_FILE_PATH, "w", encoding="utf-8") as f:
            f.write("# =====================================\n")
            f.write("# Daily Agent ç¯å¢ƒå˜é‡é…ç½®\n")
            f.write("# =====================================\n\n")
            
            # åˆ†ç»„å†™å…¥
            groups = {
                "æœåŠ¡é…ç½®": ["APP_NAME", "DEBUG", "LOG_LEVEL", "HOST", "PORT"],
                "æ•°æ®åº“é…ç½®": ["DATABASE_URL"],
                "LLM é…ç½®": ["LLM_PROVIDER", "LLM_API_KEY", "LLM_BASE_URL", "LLM_MODEL", 
                            "OPENAI_API_KEY", "OPENAI_BASE_URL", "OPENAI_MODEL"],
                "LLM åŠŸèƒ½å¼€å…³": ["ENABLE_SUMMARY", "ENABLE_QUALITY_CHECK", "ENABLE_TAGGING", 
                                "ENABLE_RECOMMENDATION", "SUMMARY_LENGTH"],
                "é‡‡é›†é…ç½®": ["MAX_CONCURRENT_COLLECTORS", "REQUEST_DELAY", "CONTENT_RETENTION_DAYS"],
                "æ¨é€é…ç½®": ["DEFAULT_PUSH_TIME", "TIMEZONE"],
                "Telegram": ["TELEGRAM_BOT_TOKEN", "TELEGRAM_CHAT_ID"],
                "Slack": ["SLACK_BOT_TOKEN", "SLACK_CHANNEL"],
                "Discord": ["DISCORD_BOT_TOKEN", "DISCORD_CHANNEL_ID"],
                "é‚®ä»¶": ["SMTP_HOST", "SMTP_PORT", "SMTP_USER", "SMTP_PASSWORD", "EMAIL_FROM", "EMAIL_TO"],
                "å®‰å…¨": ["API_SECRET_KEY"],
            }
            
            written_keys = set()
            for group_name, keys in groups.items():
                f.write(f"# ===== {group_name} =====\n")
                for key in keys:
                    if key in env_content:
                        f.write(f"{key}={env_content[key]}\n")
                        written_keys.add(key)
                f.write("\n")
            
            # å†™å…¥å…¶ä»–æœªåˆ†ç»„çš„é…ç½®
            other_keys = set(env_content.keys()) - written_keys
            if other_keys:
                f.write("# ===== å…¶ä»–é…ç½® =====\n")
                for key in sorted(other_keys):
                    f.write(f"{key}={env_content[key]}\n")
        
        # è®¾ç½®æ–‡ä»¶æƒé™ï¼ˆä»…é™ Unixï¼‰
        try:
            os.chmod(self.ENV_FILE_PATH, 0o600)
        except Exception:
            pass
        
        # æ¸…é™¤ settings ç¼“å­˜ï¼Œç¡®ä¿é‡æ–°åŠ è½½é…ç½®
        from src.config import get_settings
        get_settings.cache_clear()
    
    async def test_connection(self, config: Optional[LLMConfig] = None) -> Tuple[bool, str]:
        """æµ‹è¯• LLM è¿æ¥"""
        test_config = config or self.config
        
        if not test_config.is_configured():
            return False, "æœªé…ç½® LLM"
        
        if test_config.provider == "skip":
            return True, "å·²è·³è¿‡ LLM é…ç½®"
        
        if test_config.provider == "ollama":
            return await self._test_ollama(test_config)
        
        return await self._test_openai_compatible(test_config)
    
    async def _test_ollama(self, config: LLMConfig) -> Tuple[bool, str]:
        """æµ‹è¯• Ollama è¿æ¥"""
        base_url = config.base_url or "http://localhost:11434"
        
        console.print(f"  [dim]â†’ è¿æ¥åœ°å€: {base_url}[/dim]")
        console.print(f"  [dim]â†’ æµ‹è¯•æ¨¡å‹: {config.model}[/dim]")
        
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                console.print("  [dim]â†’ è·å–æ¨¡å‹åˆ—è¡¨...[/dim]")
                
                # æµ‹è¯•æœåŠ¡æ˜¯å¦è¿è¡Œ
                response = await client.get(f"{base_url}/api/tags")
                console.print(f"  [dim]â†’ å“åº”çŠ¶æ€: {response.status_code}[/dim]")
                
                if response.status_code != 200:
                    return False, f"Ollama æœåŠ¡è¿”å›é”™è¯¯: {response.status_code}"
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
                data = response.json()
                models = [m.get("name", "") for m in data.get("models", [])]
                console.print(f"  [dim]â†’ å·²å®‰è£…æ¨¡å‹: {', '.join(models[:5])}[/dim]")
                
                if config.model not in models:
                    return False, f"æ¨¡å‹ {config.model} æœªæ‰¾åˆ°ã€‚å·²å®‰è£…æ¨¡å‹: {', '.join(models[:5])}"
                
                console.print("  [dim]â†’ æµ‹è¯•æ¨¡å‹ç”Ÿæˆ...[/dim]")
                
                # ç®€å•æµ‹è¯•ç”Ÿæˆ
                test_response = await client.post(
                    f"{base_url}/api/generate",
                    json={"model": config.model, "prompt": "Hi", "stream": False},
                    timeout=30.0
                )
                
                console.print(f"  [dim]â†’ ç”Ÿæˆæµ‹è¯•çŠ¶æ€: {test_response.status_code}[/dim]")
                
                if test_response.status_code == 200:
                    return True, f"Ollama è¿æ¥æ­£å¸¸ï¼Œæ¨¡å‹ {config.model} å¯ç”¨"
                else:
                    return False, f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {test_response.status_code}"
                    
        except httpx.ConnectError as e:
            console.print(f"  [dim]â†’ è¿æ¥å¤±è´¥: {e}[/dim]")
            return False, f"æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡ ({base_url})ï¼Œè¯·ç¡®è®¤æœåŠ¡å·²å¯åŠ¨"
        except Exception as e:
            console.print(f"  [dim]â†’ å¼‚å¸¸: {type(e).__name__}: {e}[/dim]")
            return False, f"æµ‹è¯•å¤±è´¥: {str(e)}"
    
    async def _test_openai_compatible(self, config: LLMConfig) -> Tuple[bool, str]:
        """æµ‹è¯• OpenAI å…¼å®¹ API"""
        if not config.api_key:
            return False, "æœªè®¾ç½® API Key"
        
        base_url = config.base_url or "https://api.openai.com/v1"
        
        console.print(f"  [dim]â†’ ä½¿ç”¨ API åœ°å€: {base_url}[/dim]")
        console.print(f"  [dim]â†’ æµ‹è¯•æ¨¡å‹: {config.model}[/dim]")
        console.print(f"  [dim]â†’ æä¾›å•†: {config.provider}[/dim]")
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                headers = {"Authorization": f"Bearer {config.api_key}"}
                
                # Azure ç‰¹æ®Šå¤„ç†
                if config.provider == "azure":
                    headers["api-key"] = config.api_key
                    url = f"{base_url}/openai/deployments/{config.deployment}/chat/completions?api-version={config.api_version or '2024-02-15-preview'}"
                else:
                    url = f"{base_url}/chat/completions"
                
                console.print(f"  [dim]â†’ è¯·æ±‚ URL: {url}[/dim]")
                
                # æ„å»ºè¯·æ±‚ä½“
                request_body = {
                    "model": config.model,
                    "messages": [{"role": "user", "content": "Hi"}],
                    "max_tokens": 5
                }
                console.print(f"  [dim]â†’ è¯·æ±‚ä½“: {request_body}[/dim]")
                console.print("  [dim]â†’ å‘é€è¯·æ±‚...[/dim]")
                
                response = await client.post(
                    url,
                    headers=headers,
                    json=request_body
                )
                
                console.print(f"  [dim]â†’ å“åº”çŠ¶æ€: {response.status_code}[/dim]")
                
                if response.status_code == 200:
                    data = response.json()
                    if "choices" in data and len(data["choices"]) > 0:
                        content = data["choices"][0].get("message", {}).get("content", "")
                        console.print(f"  [dim]â†’ å“åº”å†…å®¹: {content[:50]}...[/dim]")
                    return True, f"API è¿æ¥æ­£å¸¸ï¼Œæ¨¡å‹ {config.model} å¯ç”¨"
                elif response.status_code == 401:
                    console.print("  [dim]â†’ é”™è¯¯: API Key è®¤è¯å¤±è´¥[/dim]")
                    return False, "API Key æ— æ•ˆæˆ–å·²è¿‡æœŸ"
                elif response.status_code == 404:
                    console.print(f"  [dim]â†’ é”™è¯¯: æ¨¡å‹æœªæ‰¾åˆ°[/dim]")
                    console.print(f"  [dim]â†’ å“åº”è¯¦æƒ…: {response.text[:200]}[/dim]")
                    return False, f"æ¨¡å‹ {config.model} ä¸å­˜åœ¨"
                else:
                    error_msg = response.text[:200]
                    console.print(f"  [dim]â†’ é”™è¯¯å“åº”: {error_msg}[/dim]")
                    return False, f"API é”™è¯¯ ({response.status_code}): {error_msg}"
                    
        except httpx.ConnectError as e:
            console.print(f"  [dim]â†’ è¿æ¥å¤±è´¥: {e}[/dim]")
            return False, f"æ— æ³•è¿æ¥åˆ° API æœåŠ¡ ({base_url})"
        except httpx.TimeoutException:
            console.print("  [dim]â†’ è¯·æ±‚è¶…æ—¶[/dim]")
            return False, "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        except Exception as e:
            console.print(f"  [dim]â†’ å¼‚å¸¸: {type(e).__name__}: {e}[/dim]")
            return False, f"æµ‹è¯•å¤±è´¥: {str(e)}"


class LLMSetupWizard:
    """LLM é…ç½®å‘å¯¼"""
    
    def __init__(self):
        self.manager = LLMConfigManager()
        self.config = LLMConfig()
    
    async def run_setup(self):
        """è¿è¡Œé…ç½®å‘å¯¼"""
        self._print_welcome()
        
        # æ­¥éª¤ 1: é€‰æ‹©æä¾›å•†
        provider = await self._select_provider()
        if provider == LLMProvider.SKIP:
            self.config.provider = "skip"
            self.config.model = ""
            await self._save_and_finish()
            return
        
        self.config.provider = provider.value
        
        # æ­¥éª¤ 2: é…ç½® API
        success = await self._configure_api(provider)
        if not success:
            console.print("\n[yellow]âš ï¸ é…ç½®å·²å–æ¶ˆ[/yellow]")
            return
        
        # æ­¥éª¤ 3: åŠŸèƒ½é…ç½®
        await self._configure_features()
        
        # ä¿å­˜é…ç½®
        await self._save_and_finish()
    
    def _print_welcome(self):
        """æ‰“å°æ¬¢è¿ä¿¡æ¯"""
        console.print(Panel(
            "[bold green]ğŸ¤– LLM é…ç½®å‘å¯¼[/bold green]\n\n"
            "æœ¬å‘å¯¼å°†å¸®åŠ©æ‚¨é…ç½®å¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äº:\n"
            "  â€¢ æ™ºèƒ½å†…å®¹æ‘˜è¦ç”Ÿæˆ\n"
            "  â€¢ å†…å®¹è´¨é‡è¯„ä¼°\n"
            "  â€¢ ä¸ªæ€§åŒ–æ¨èä¼˜åŒ–\n\n"
            "[dim]æ”¯æŒ: OpenAI, Gemini, Kimi, Qwen, GLM, OpenRouter, Ollama ç­‰[/dim]",
            title="LLM é…ç½®",
            border_style="blue"
        ))
        
        Prompt.ask("\næŒ‰ Enter å¼€å§‹é…ç½®")
    
    # åœ¨å‘å¯¼ä¸­æ˜¾ç¤ºçš„æä¾›å•†ï¼ˆæ’é™¤ Ollama, Azure, Baidu, æ—§ç‰ˆ Aliyun/Zhipuï¼‰
    WIZARD_PROVIDERS = [
        LLMProvider.OPENAI,
        LLMProvider.GEMINI,
        LLMProvider.MOONSHOT,
        LLMProvider.QWEN,
        LLMProvider.GLM,
        LLMProvider.OPENROUTER,
    ]
    
    async def _select_provider(self) -> LLMProvider:
        """é€‰æ‹© LLM æä¾›å•†"""
        console.print("\n[bold blue]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold blue]")
        console.print("[bold blue]æ­¥éª¤ 1/3: é€‰æ‹© LLM æä¾›å•†[/bold blue]")
        console.print("[bold blue]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold blue]\n")
        
        console.print("[bold]ğŸ“ é€‰æ‹© LLM æä¾›å•†ï¼š[/bold]\n")
        
        providers = self.WIZARD_PROVIDERS
        for i, provider in enumerate(providers, 1):
            config = PROVIDER_CONFIGS[provider]
            marker = "â˜…" if i <= 3 else " "
            console.print(f"   [{marker}] [{i}] {config.emoji} {config.display_name}")
            console.print(f"       [dim]{config.description}[/dim]\n")
        
        console.print(f"   [ ] [{len(providers) + 1}] â­ï¸  è·³è¿‡ - æš‚ä¸é…ç½® LLM")
        console.print("       [dim]å°†ä½¿ç”¨è§„åˆ™æ‘˜è¦ï¼ˆåŠŸèƒ½å—é™ï¼‰[/dim]\n")
        
        choice = IntPrompt.ask(
            "è¯·é€‰æ‹©",
            choices=[str(i) for i in range(1, len(providers) + 2)],
            default=1
        )
        
        if choice == len(providers) + 1:
            return LLMProvider.SKIP
        
        return providers[choice - 1]
    
    async def _configure_api(self, provider: LLMProvider) -> bool:
        """é…ç½® API"""
        console.print("\n[bold blue]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold blue]")
        console.print("[bold blue]æ­¥éª¤ 2/3: é…ç½® API[/bold blue]")
        console.print("[bold blue]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold blue]\n")
        
        config = PROVIDER_CONFIGS[provider]
        
        console.print(f"[bold]ğŸ“ æ‚¨é€‰æ‹©äº† [{config.display_name}][/bold]\n")
        console.print(f"[dim]{config.help_text}[/dim]\n")
        
        # API Key
        if config.requires_api_key:
            console.print("[yellow]âš ï¸  æç¤ºï¼šå¯†é’¥ä»…ä¿å­˜åœ¨æœ¬åœ° .env æ–‡ä»¶ï¼Œä¸ä¼šä¸Šä¼ [/yellow]\n")
            
            api_key = Prompt.ask(f"è¯·è¾“å…¥ {config.display_name} API Key")
            
            if not api_key or not api_key.strip():
                console.print("[red]âœ— API Key ä¸èƒ½ä¸ºç©º[/red]")
                return False
            
            api_key = api_key.strip()
            
            # ç®€å•éªŒè¯æ ¼å¼
            if not self._validate_api_key_format(provider, api_key):
                if not Confirm.ask("API Key æ ¼å¼çœ‹èµ·æ¥ä¸æ­£ç¡®ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ", default=False):
                    return False
            
            self.config.api_key = api_key
            console.print("[green]âœ… API Key æ ¼å¼éªŒè¯é€šè¿‡[/green]\n")
        
        # Base URL é…ç½® - æ‰€æœ‰æä¾›å•†éƒ½å¯è‡ªå®šä¹‰
        default_url = config.base_url_hint or ""
        
        console.print("\n[bold]ğŸ“ é…ç½® API åœ°å€ï¼š[/bold]\n")
        console.print(f"[dim]é»˜è®¤åœ°å€: {default_url}[/dim]\n")
        
        custom_url = Prompt.ask("è¯·è¾“å…¥ API åœ°å€", default=default_url)
        if custom_url and custom_url != default_url:
            self.config.base_url = custom_url
        else:
            self.config.base_url = default_url
        
        console.print(f"[green]âœ… å·²é…ç½® API åœ°å€: {self.config.base_url}[/green]")
        
        # è¾“å…¥æ¨¡å‹åç§°
        console.print("\n[bold]ğŸ“ é…ç½®æ¨¡å‹ï¼š[/bold]\n")
        
        # æ˜¾ç¤ºæ¨èçš„æ¨¡å‹ä½œä¸ºå‚è€ƒ
        console.print("[dim]æ¨èçš„æ¨¡å‹:[/dim]")
        for model in config.models[:3]:
            rec = " â˜…æ¨è" if model.recommended else ""
            console.print(f"  â€¢ {model.id} - {model.description}{rec}")
        console.print("")
        
        # è®©ç”¨æˆ·è‡ªè¡Œå¡«å…¥æ¨¡å‹åç§°
        default_model = config.models[0].id if config.models else ""
        model_input = Prompt.ask("è¯·è¾“å…¥æ¨¡å‹åç§°", default=default_model)
        
        self.config.model = model_input.strip()
        console.print(f"[green]âœ… å·²é…ç½®æ¨¡å‹: {self.config.model}[/green]")
        
        return True
    
    def _validate_api_key_format(self, provider: LLMProvider, api_key: str) -> bool:
        """éªŒè¯ API Key æ ¼å¼"""
        if provider == LLMProvider.OPENAI:
            # OpenAI key é€šå¸¸ä»¥ sk- å¼€å¤´
            return api_key.startswith("sk-") and len(api_key) > 20
        elif provider == LLMProvider.OPENROUTER:
            # OpenRouter key é€šå¸¸ä»¥ sk-or- å¼€å¤´
            return api_key.startswith("sk-or-") and len(api_key) > 20
        elif provider == LLMProvider.AZURE:
            # Azure key æ˜¯ 32 ä½åå…­è¿›åˆ¶
            return len(api_key) == 32 and all(c in "0123456789abcdef" for c in api_key.lower())
        elif provider == LLMProvider.GEMINI:
            # Gemini key é€šå¸¸ä»¥ AIza å¼€å¤´
            return api_key.startswith("AIza") and len(api_key) > 20
        elif provider == LLMProvider.MOONSHOT:
            # Moonshot key é€šå¸¸ä»¥ sk- å¼€å¤´
            return api_key.startswith("sk-") and len(api_key) > 20
        elif provider == LLMProvider.QWEN:
            # Qwen key é€šå¸¸ä»¥ sk- å¼€å¤´
            return api_key.startswith("sk-") and len(api_key) > 20
        elif provider == LLMProvider.GLM:
            # GLM key é€šå¸¸æ˜¯ä¸€ä¸²è¾ƒé•¿çš„å­—æ¯æ•°å­—æ··åˆ
            return len(api_key) >= 16
        return True  # å…¶ä»–æä¾›å•†ä¸åšä¸¥æ ¼éªŒè¯
    
    async def _configure_features(self):
        """é…ç½®åŠŸèƒ½å¼€å…³"""
        console.print("\n[bold blue]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold blue]")
        console.print("[bold blue]æ­¥éª¤ 3/3: åŠŸèƒ½é…ç½®[/bold blue]")
        console.print("[bold blue]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold blue]\n")
        
        console.print("[bold]ğŸ“ å¯ç”¨ LLM å¢å¼ºåŠŸèƒ½ï¼š[/bold]\n")
        
        self.config.enable_summary = Confirm.ask(
            "   [x] æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ\n       [dim]ä½¿ç”¨ LLM ç”Ÿæˆé«˜è´¨é‡å†…å®¹æ‘˜è¦[/dim]",
            default=True
        )
        
        self.config.enable_quality_check = Confirm.ask(
            "\n   [x] å†…å®¹è´¨é‡è¯„ä¼°\n       [dim]è‡ªåŠ¨è¯„ä¼°æ–‡ç« åŸåˆ›æ€§ã€æ·±åº¦[/dim]",
            default=True
        )
        
        self.config.enable_tagging = Confirm.ask(
            "\n   [ ] æ™ºèƒ½æ ‡ç­¾æå–\n       [dim]è‡ªåŠ¨æå–ç²¾å‡†å†…å®¹æ ‡ç­¾[/dim]",
            default=False
        )
        
        self.config.enable_recommendation = Confirm.ask(
            "\n   [ ] ä¸ªæ€§åŒ–æ¨èä¼˜åŒ–\n       [dim]åŸºäº LLM çš„ä¸ªæ€§åŒ–æ’åº[/dim]",
            default=False
        )
        
        # æ‘˜è¦é•¿åº¦
        console.print("\n[bold]ğŸ“ æ‘˜è¦é•¿åº¦åå¥½ï¼š[/bold]")
        console.print("   [1] ç®€æ´ - ä¸€å¥è¯æ‘˜è¦")
        console.print("   [2] æ ‡å‡† - 3-5ä¸ªè¦ç‚¹ [dim](æ¨è)[/dim]")
        console.print("   [3] è¯¦ç»† - å®Œæ•´æ®µè½æ‘˜è¦")
        
        length_choice = IntPrompt.ask("è¯·é€‰æ‹©", choices=["1", "2", "3"], default=2)
        self.config.summary_length = {1: "short", 2: "medium", 3: "long"}[length_choice]
    
    async def _save_and_finish(self):
        """ä¿å­˜é…ç½®å¹¶å®Œæˆ"""
        # æ˜¾ç¤ºé…ç½®é¢„è§ˆ
        console.print("\n[bold blue]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold blue]")
        console.print("[bold blue]é…ç½®é¢„è§ˆ[/bold blue]")
        console.print("[bold blue]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold blue]\n")
        
        if self.config.provider == "skip":
            console.print("  é…ç½®: [yellow]è·³è¿‡ LLM é…ç½®[/yellow]")
            console.print("  è¯´æ˜: å°†ä½¿ç”¨è§„åˆ™æ‘˜è¦ï¼ˆåŠŸèƒ½å—é™ï¼‰\n")
        else:
            provider_config = PROVIDER_CONFIGS.get(LLMProvider(self.config.provider))
            if provider_config:
                console.print(f"  æä¾›å•†: {provider_config.emoji} {provider_config.display_name}")
            
            console.print(f"  æ¨¡å‹: {self.config.model}")
            console.print(f"  API Key: {self.config.get_masked_api_key()}")
            
            if self.config.base_url and self.config.provider != "ollama":
                console.print(f"  è‡ªå®šä¹‰åœ°å€: {self.config.base_url}")
            
            features = []
            if self.config.enable_summary:
                features.append("æ™ºèƒ½æ‘˜è¦")
            if self.config.enable_quality_check:
                features.append("è´¨é‡è¯„ä¼°")
            if self.config.enable_tagging:
                features.append("æ™ºèƒ½æ ‡ç­¾")
            if self.config.enable_recommendation:
                features.append("æ¨èä¼˜åŒ–")
            
            console.print(f"  åŠŸèƒ½: {', '.join(features) if features else 'æ— '}")
            
            length_map = {"short": "ç®€æ´", "medium": "æ ‡å‡†", "long": "è¯¦ç»†"}
            console.print(f"  æ‘˜è¦é•¿åº¦: {length_map.get(self.config.summary_length, 'æ ‡å‡†')}")
        
        console.print("")
        
        if not Confirm.ask("æ˜¯å¦ä¿å­˜é…ç½®?", default=True):
            console.print("\n[yellow]âš ï¸ é…ç½®æœªä¿å­˜[/yellow]")
            return
        
        # ä¿å­˜é…ç½®
        self.manager.save_config(self.config)
        
        # æµ‹è¯•è¿æ¥
        if self.config.provider != "skip":
            console.print("\n[bold]ğŸ§ª æ­£åœ¨æµ‹è¯• API è¿æ¥...[/bold]")
            
            success, message = await self.manager.test_connection(self.config)
            
            if success:
                console.print(f"[green]âœ… {message}[/green]")
            else:
                console.print(f"[yellow]âš ï¸ {message}[/yellow]")
                console.print("[yellow]   é…ç½®å·²ä¿å­˜ï¼Œä½†å¯èƒ½æ— æ³•æ­£å¸¸ä½¿ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®[/yellow]")
        
        # å®Œæˆä¿¡æ¯
        console.print("\n[bold green]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold green]")
        console.print("[bold green]âœ… LLM é…ç½®å®Œæˆï¼[/bold green]")
        console.print("[bold green]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold green]\n")
        
        console.print("é…ç½®å·²ä¿å­˜åˆ° .env æ–‡ä»¶\n")
        
        console.print("[bold]ğŸ’¡ æç¤ºï¼š[/bold]")
        console.print("  â€¢ è¿è¡Œ [cyan]python -m src.cli llm status[/cyan] æŸ¥çœ‹é…ç½®çŠ¶æ€")
        console.print("  â€¢ è¿è¡Œ [cyan]python -m src.cli llm test[/cyan] æµ‹è¯•è¿æ¥")
        console.print("  â€¢ å¦‚éœ€æ›´æ”¹é…ç½®ï¼Œé‡æ–°è¿è¡Œ [cyan]python -m src.cli llm setup[/cyan]")
    
    async def switch_model(self):
        """åˆ‡æ¢æ¨¡å‹"""
        current_config = self.manager.get_current_config()
        
        if not current_config.is_configured():
            console.print("[yellow]âš ï¸ å°šæœªé…ç½® LLMï¼Œè¯·å…ˆè¿è¡Œ: python -m src.cli llm setup[/yellow]")
            return
        
        provider = LLMProvider(current_config.provider)
        provider_config = PROVIDER_CONFIGS.get(provider)
        
        if not provider_config:
            console.print("[red]âœ— å½“å‰æä¾›å•†ä¸æ”¯æŒåˆ‡æ¢æ¨¡å‹[/red]")
            return
        
        console.print(f"\n[bold]ğŸ“ é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹ï¼š[/bold]\n")
        console.print(f"å½“å‰: {current_config.model}\n")
        
        models = provider_config.models
        for i, model in enumerate(models, 1):
            marker = "âœ“" if model.id == current_config.model else " "
            console.print(f"   [{marker}] [{i}] {model.name} - {model.description}")
        
        console.print(f"   [ ] [{len(models) + 1}] é…ç½®æ–°æ¨¡å‹...")
        
        choice = IntPrompt.ask(
            "\nè¯·é€‰æ‹©",
            choices=[str(i) for i in range(1, len(models) + 2)],
            default=1
        )
        
        if choice == len(models) + 1:
            # é‡æ–°è¿è¡Œé…ç½®å‘å¯¼
            await self.run_setup()
            return
        
        new_model = models[choice - 1]
        current_config.model = new_model.id
        
        self.manager.save_config(current_config)
        console.print(f"\n[green]âœ… å·²åˆ‡æ¢åˆ° {new_model.name}[/green]")
        
        # æµ‹è¯•æ–°æ¨¡å‹
        console.print("\n[bold]ğŸ§ª æµ‹è¯•æ–°æ¨¡å‹...[/bold]")
        success, message = await self.manager.test_connection(current_config)
        
        if success:
            console.print(f"[green]âœ… {message}[/green]")
        else:
            console.print(f"[yellow]âš ï¸ {message}[/yellow]")
    
    def print_status(self):
        """æ‰“å°é…ç½®çŠ¶æ€"""
        config = self.manager.get_current_config()
        
        console.print("\n[bold blue]ğŸ¤– LLM é…ç½®çŠ¶æ€[/bold blue]")
        console.print("[bold blue]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold blue]\n")
        
        if not config.is_configured():
            console.print("  çŠ¶æ€: [yellow]âš ï¸ æœªé…ç½®[/yellow]\n")
            console.print("  è¿è¡Œ [cyan]python -m src.cli llm setup[/cyan] è¿›è¡Œé…ç½®")
            return
        
        if config.provider == "skip":
            console.print("  é…ç½®: [yellow]å·²è·³è¿‡ LLM é…ç½®[/yellow]")
            console.print("  è¯´æ˜: ä½¿ç”¨è§„åˆ™æ‘˜è¦ï¼ˆåŠŸèƒ½å—é™ï¼‰\n")
            return
        
        # æä¾›å•†ä¿¡æ¯
        provider = LLMProvider(config.provider)
        provider_config = PROVIDER_CONFIGS.get(provider)
        
        if provider_config:
            console.print(f"  æä¾›å•†:   {provider_config.emoji} {provider_config.display_name}")
        else:
            console.print(f"  æä¾›å•†:   {config.provider}")
        
        console.print(f"  æ¨¡å‹:     {config.model}")
        console.print(f"  API Key:  {config.get_masked_api_key()}")
        
        # æµ‹è¯•è¿æ¥
        import asyncio
        success, message = asyncio.run(self.manager.test_connection())
        
        if success:
            console.print(f"  çŠ¶æ€:     [green]âœ… æ­£å¸¸[/green]")
        else:
            console.print(f"  çŠ¶æ€:     [red]âœ— {message}[/red]")
        
        console.print("")
        
        # åŠŸèƒ½çŠ¶æ€
        console.print("[bold]åŠŸèƒ½çŠ¶æ€ï¼š[/bold]")
        summary_status = "âœ… å·²å¯ç”¨" if config.enable_summary else "âšª æœªå¯ç”¨"
        quality_status = "âœ… å·²å¯ç”¨" if config.enable_quality_check else "âšª æœªå¯ç”¨"
        tagging_status = "âœ… å·²å¯ç”¨" if config.enable_tagging else "âšª æœªå¯ç”¨"
        rec_status = "âœ… å·²å¯ç”¨" if config.enable_recommendation else "âšª æœªå¯ç”¨"
        
        console.print(f"  æ™ºèƒ½æ‘˜è¦:   {summary_status}")
        console.print(f"  è´¨é‡è¯„ä¼°:   {quality_status}")
        console.print(f"  æ™ºèƒ½æ ‡ç­¾:   {tagging_status}")
        console.print(f"  æ¨èä¼˜åŒ–:   {rec_status}")
        
        console.print("\n[bold blue]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold blue]\n")
    
    def print_models(self):
        """æ‰“å°æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        console.print("\n[bold blue]æ”¯æŒçš„ LLM æä¾›å•†å’Œæ¨¡å‹[/bold blue]")
        console.print("[bold blue]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold blue]\n")
        
        for provider in LLMProvider:
            if provider == LLMProvider.SKIP:
                continue
            
            config = PROVIDER_CONFIGS[provider]
            
            console.print(Panel(
                f"[bold]{config.emoji} {config.display_name}[/bold]\n"
                f"[dim]{config.description}[/dim]\n\n"
                f"[bold]å¯ç”¨æ¨¡å‹ï¼š[/bold]\n" +
                "\n".join([
                    f"  â€¢ {m.name} - {m.description} [dim]({m.price_hint})[/dim]"
                    for m in config.models
                ]),
                border_style="green"
            ))


# å…¨å±€ç®¡ç†å™¨å®ä¾‹
_llm_manager: Optional[LLMConfigManager] = None


def get_llm_manager() -> LLMConfigManager:
    """è·å– LLM é…ç½®ç®¡ç†å™¨å•ä¾‹"""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMConfigManager()
    return _llm_manager

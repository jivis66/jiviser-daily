"""
LLM è¾…åŠ©ä¸“å®¶é…ç½®æ¨¡å¼
å…ˆé…ç½® LLMï¼Œç„¶åè®© LLM é€šè¿‡å¯¹è¯å¸®åŠ©ç”¨æˆ·å®Œæˆæ‰€æœ‰é…ç½®
"""
import json
import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional

import httpx
import yaml
from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel
from rich.prompt import Confirm, Prompt
from rich.spinner import Spinner
from rich.syntax import Syntax
from rich.tree import Tree

console = Console()


@dataclass
class LLMConfig:
    """LLM é…ç½®"""
    provider: str
    api_key: str
    model: str
    base_url: Optional[str] = None


class LLMAssistedSetup:
    """LLM è¾…åŠ©é…ç½®å‘å¯¼"""

    def __init__(self):
        self.llm_config: Optional[LLMConfig] = None
        self.config_dir = Path(__file__).parent.parent / "config"
        self.data_dir = Path(__file__).parent.parent / "data"
        self.conversation_history: List[Dict] = []

    async def run(self):
        """è¿è¡Œä¸“å®¶é…ç½®æ¨¡å¼"""
        console.print(Panel.fit(
            "[bold blue]ğŸ¯ LLM è¾…åŠ©ä¸“å®¶é…ç½®æ¨¡å¼[/bold blue]\n\n"
            "æ­¤æ¨¡å¼å°†:\n"
            "1. å…ˆé…ç½® LLMï¼ˆç”¨äºæ™ºèƒ½è¾…åŠ©ï¼‰\n"
            "2. ç„¶åä¸ AI å¯¹è¯å®Œæˆæ‰€æœ‰é…ç½®\n\n"
            "AI ä¼šå¸®ä½ :\n"
            "â€¢ åˆ†æä½ çš„éœ€æ±‚\n"
            "â€¢ æ¨èåˆé€‚çš„æ•°æ®æº\n"
            "â€¢ è§£é‡Šé…ç½®é€‰é¡¹\n"
            "â€¢ è‡ªåŠ¨ç”Ÿæˆé…ç½®",
            border_style="blue"
        ))

        # æ­¥éª¤1ï¼šé…ç½® LLM
        if not await self._setup_llm():
            console.print("[red]âœ— LLM é…ç½®å¤±è´¥ï¼Œæ— æ³•è¿›å…¥ä¸“å®¶æ¨¡å¼[/red]")
            return False

        # æ­¥éª¤2ï¼šLLM è¾…åŠ©é…ç½®
        await self._llm_assisted_configuration()

        return True

    async def _setup_llm(self) -> bool:
        """é…ç½® LLM"""
        console.print("\n[bold cyan]æ­¥éª¤ 1/2: é…ç½® LLM[/bold cyan]")
        console.print("é¦–å…ˆéœ€è¦é…ç½®ä¸€ä¸ª LLM æ¥è¾…åŠ©åç»­é…ç½®\n")

        # é€‰æ‹©æä¾›å•†
        providers = [
            ("1", "openai", "ğŸŒ OpenAI", "ç¨³å®šé«˜è´¨é‡"),
            ("2", "moonshot", "ğŸŒ™ Kimi", "ä¸­æ–‡é•¿æ–‡æœ¬"),
            ("3", "qwen", "ğŸ¤– é€šä¹‰åƒé—®", "ä¸­æ–‡ä¼˜åŒ–"),
            ("4", "openrouter", "ğŸ”— OpenRouter", "å¤šæ¨¡å‹æ¥å…¥"),
            ("5", "ollama", "ğŸ  Ollama", "æœ¬åœ°éƒ¨ç½²"),
        ]

        console.print("[bold]é€‰æ‹© LLM æä¾›å•†:[/bold]")
        for num, key, name, desc in providers:
            console.print(f"  [{num}] {name} - {desc}")

        choice = Prompt.ask("è¯·é€‰æ‹©", choices=[p[0] for p in providers], default="1")
        provider = providers[int(choice) - 1][1]

        # è¾“å…¥ API Key
        api_key = Prompt.ask(f"è¯·è¾“å…¥ {provider} API Key", password=True)

        if not api_key:
            console.print("[red]API Key ä¸èƒ½ä¸ºç©º[/red]")
            return False

        # é€‰æ‹©æ¨¡å‹
        model = await self._select_model(provider)

        # å¯é€‰ï¼šè‡ªå®šä¹‰ base_url
        base_url = None
        if Confirm.ask("æ˜¯å¦éœ€è¦è‡ªå®šä¹‰ API åœ°å€?", default=False):
            base_url = Prompt.ask("è¯·è¾“å…¥ API åŸºç¡€ URL")

        # æµ‹è¯•è¿æ¥
        self.llm_config = LLMConfig(
            provider=provider,
            api_key=api_key,
            model=model,
            base_url=base_url
        )

        with console.status("[bold green]æ­£åœ¨æµ‹è¯• LLM è¿æ¥..."):
            if await self._test_llm():
                console.print(f"[green]âœ“ LLM é…ç½®æˆåŠŸ: {model}[/green]\n")
                return True
            else:
                console.print("[red]âœ— LLM è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key[/red]")
                return False

    async def _select_model(self, provider: str) -> str:
        """é€‰æ‹©æ¨¡å‹"""
        models_map = {
            "openai": [
                ("gpt-4o-mini", "GPT-4o-mini (æ¨èï¼Œæ€§ä»·æ¯”é«˜)"),
                ("gpt-4o", "GPT-4o (æœ€å¼ºæ€§èƒ½)"),
                ("gpt-4-turbo", "GPT-4 Turbo"),
            ],
            "moonshot": [
                ("moonshot-v1-128k", "Kimi K1 128K (æ¨è)"),
                ("moonshot-v1-32k", "Kimi K1 32K"),
            ],
            "qwen": [
                ("qwen-max", "Qwen Max (æ¨è)"),
                ("qwen-plus", "Qwen Plus"),
                ("qwen-turbo", "Qwen Turbo"),
            ],
            "openrouter": [
                ("openai/gpt-4o-mini", "GPT-4o-mini (æ¨è)"),
                ("anthropic/claude-3.5-sonnet", "Claude 3.5 Sonnet"),
            ],
            "ollama": [
                ("qwen2.5:14b", "Qwen2.5 14B (æ¨è)"),
                ("llama3.2", "Llama 3.2"),
            ],
        }

        models = models_map.get(provider, [("gpt-4o-mini", "Default")])

        console.print(f"\n[bold]é€‰æ‹©æ¨¡å‹:[/bold]")
        for i, (model_id, desc) in enumerate(models, 1):
            console.print(f"  [{i}] {desc}")

        choice = Prompt.ask("è¯·é€‰æ‹©", choices=[str(i) for i in range(1, len(models) + 1)], default="1")
        return models[int(choice) - 1][0]

    async def _test_llm(self) -> bool:
        """æµ‹è¯• LLM è¿æ¥"""
        try:
            headers = {
                "Authorization": f"Bearer {self.llm_config.api_key}",
                "Content-Type": "application/json"
            }

            base_url = self.llm_config.base_url or self._get_default_base_url(self.llm_config.provider)

            payload = {
                "model": self.llm_config.model,
                "messages": [{"role": "user", "content": "Hi"}],
                "max_tokens": 5
            }

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=30.0
                )
                return response.status_code == 200

        except Exception as e:
            console.print(f"[red]è¿æ¥é”™è¯¯: {e}[/red]")
            return False

    def _get_default_base_url(self, provider: str) -> str:
        """è·å–é»˜è®¤ base URL"""
        urls = {
            "openai": "https://api.openai.com/v1",
            "moonshot": "https://api.moonshot.cn/v1",
            "qwen": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "openrouter": "https://openrouter.ai/api/v1",
            "ollama": "http://localhost:11434/v1",
        }
        return urls.get(provider, "https://api.openai.com/v1")

    async def _llm_assisted_configuration(self):
        """LLM è¾…åŠ©é…ç½®ä¸»æµç¨‹"""
        console.print("\n[bold cyan]æ­¥éª¤ 2/2: LLM è¾…åŠ©é…ç½®[/bold cyan]")
        console.print("ç°åœ¨ä½ å¯ä»¥å’Œ AI å¯¹è¯æ¥å®Œæˆé…ç½®ã€‚å‘Šè¯‰æˆ‘:\n")
        console.print("â€¢ ä½ çš„èŒä¸šæˆ–è§’è‰²")
        console.print("â€¢ ä½ å…³æ³¨çš„è¯é¢˜")
        console.print("â€¢ ä½ å¸Œæœ›æ—¥æŠ¥åŒ…å«ä»€ä¹ˆå†…å®¹\n")

        # ç³»ç»Ÿæç¤ºè¯
        system_prompt = """ä½ æ˜¯ Daily Agent é…ç½®ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·é…ç½®ä¸ªæ€§åŒ–æ—¥æŠ¥ç³»ç»Ÿã€‚

é€šè¿‡å¯¹è¯äº†è§£ç”¨æˆ·éœ€æ±‚ï¼Œç„¶åç”Ÿæˆ YAML é…ç½®æ–‡ä»¶ã€‚

ä½ éœ€è¦æ”¶é›†çš„ä¿¡æ¯ï¼š
1. ç”¨æˆ·ç”»åƒï¼šèŒä¸šã€è¡Œä¸šã€ä¸“ä¸šé¢†åŸŸ
2. å…´è¶£åå¥½ï¼šå…³æ³¨çš„è¯é¢˜ã€å†…å®¹ç±»å‹åå¥½
3. æ—¥æŠ¥è®¾ç½®ï¼šåˆ†æ ã€æ•°æ®æºã€æ¨é€æ¸ é“

ç”Ÿæˆé…ç½®æ—¶è¦ï¼š
1. è§£é‡Šä½ çš„æ¨èé€»è¾‘
2. è¯¢é—®ç”¨æˆ·ç¡®è®¤
3. ç”Ÿæˆæœ‰æ•ˆçš„ columns.yaml é…ç½®

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨ä¸­æ–‡äº¤æµ
- è§£é‡ŠæŠ€æœ¯æ¦‚å¿µ
- ç»™å‡ºå…·ä½“å¯æ“ä½œçš„é€‰é¡¹"""

        self.conversation_history.append({"role": "system", "content": system_prompt})

        # åˆå§‹é—®å€™
        welcome_msg = await self._chat_with_llm(
            "è¯·å‘ç”¨æˆ·é—®å¥½å¹¶è¯¢é—®ä»–ä»¬çš„èŒä¸šå’Œå…´è¶£ï¼Œä»¥ä¾¿ä¸ºä»–ä»¬é…ç½®æ—¥æŠ¥ã€‚"
        )
        console.print(f"\n[bold green]ğŸ¤– AI:[/bold green] {welcome_msg}\n")

        # å¯¹è¯å¾ªç¯
        collected_info = {
            "profession": None,
            "interests": [],
            "content_preference": None,
            "time_available": None,
        }

        while True:
            user_input = Prompt.ask("[bold blue]ä½ [/bold blue]")

            if user_input.lower() in ["exit", "quit", "é€€å‡º", "ç»“æŸ"]:
                console.print("\n[yellow]å·²é€€å‡ºé…ç½®[/yellow]")
                break

            if user_input.lower() in ["done", "å®Œæˆ", "ok", "ç¡®è®¤"]:
                if await self._generate_configuration():
                    break
                continue

            # åˆ†æç”¨æˆ·è¾“å…¥ï¼Œæå–å…³é”®ä¿¡æ¯
            analysis_prompt = f"""ç”¨æˆ·è¾“å…¥: {user_input}

å½“å‰å·²æ”¶é›†ä¿¡æ¯: {json.dumps(collected_info, ensure_ascii=False)}

è¯·:
1. åˆ†æç”¨æˆ·è¾“å…¥ï¼Œæ›´æ–°å·²æ”¶é›†ä¿¡æ¯
2. å¦‚æœå‘ç°æ–°çš„ä¿¡æ¯ï¼Œç¡®è®¤å¹¶è®°å½•
3. å¦‚æœä¿¡æ¯è¿˜ä¸å¤Ÿç”Ÿæˆé…ç½®ï¼Œç»§ç»­å‹å¥½åœ°è¯¢é—®
4. å¦‚æœä¿¡æ¯è¶³å¤Ÿï¼Œå¯ä»¥è¯´"ç°åœ¨å¯ä»¥ç”Ÿæˆé…ç½®äº†ï¼Œè¾“å…¥ 'å®Œæˆ' ç¡®è®¤"

ç›´æ¥å›å¤ç”¨æˆ·ï¼Œä¿æŒå¯¹è¯è‡ªç„¶ã€‚"""

            response = await self._chat_with_llm(analysis_prompt)
            console.print(f"\n[bold green]ğŸ¤– AI:[/bold green] {response}\n")

            # å°è¯•æå–ä¿¡æ¯ï¼ˆç®€å•è§„åˆ™ï¼‰
            if any(word in user_input for word in ["å¼€å‘", "ç¨‹åºå‘˜", "å·¥ç¨‹å¸ˆ", "æŠ€æœ¯"]):
                collected_info["profession"] = "tech_developer"
            elif any(word in user_input for word in ["äº§å“", "PM", "ç»ç†"]):
                collected_info["profession"] = "product_manager"
            elif any(word in user_input for word in ["æŠ•èµ„", "åˆ†æå¸ˆ", "é‡‘è"]):
                collected_info["profession"] = "investor"

            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ç”Ÿæˆé…ç½®
            if collected_info["profession"] and len(self.conversation_history) > 5:
                console.print("[dim]ğŸ’¡ æç¤º: è¾“å…¥ 'å®Œæˆ' è®© AI ç”Ÿæˆé…ç½®[/dim]\n")

    async def _chat_with_llm(self, message: str) -> str:
        """ä¸ LLM å¯¹è¯"""
        self.conversation_history.append({"role": "user", "content": message})

        try:
            headers = {
                "Authorization": f"Bearer {self.llm_config.api_key}",
                "Content-Type": "application/json"
            }

            base_url = self.llm_config.base_url or self._get_default_base_url(self.llm_config.provider)

            payload = {
                "model": self.llm_config.model,
                "messages": self.conversation_history[-10:],  # ä¿æŒä¸Šä¸‹æ–‡
                "temperature": 0.7,
                "max_tokens": 2000
            }

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=60.0
                )

                result = response.json()
                assistant_msg = result["choices"][0]["message"]["content"]
                self.conversation_history.append({"role": "assistant", "content": assistant_msg})

                return assistant_msg

        except Exception as e:
            return f"æŠ±æ­‰ï¼Œä¸ AI é€šä¿¡å‡ºé”™: {e}"

    async def _generate_configuration(self) -> bool:
        """ç”Ÿæˆæœ€ç»ˆé…ç½®"""
        console.print("\n[bold cyan]æ­£åœ¨ç”Ÿæˆé…ç½®...[/bold cyan]\n")

        generate_prompt = """åŸºäºä»¥ä¸Šå¯¹è¯ï¼Œè¯·ç”Ÿæˆå®Œæ•´çš„ columns.yaml é…ç½®ã€‚

è¦æ±‚:
1. å…ˆæ€»ç»“ç”¨æˆ·çš„éœ€æ±‚
2. è§£é‡Šä½ çš„é…ç½®é€»è¾‘
3. ç”Ÿæˆæœ‰æ•ˆçš„ YAML é…ç½®ä»£ç 

é…ç½®ç»“æ„:
```yaml
columns:
  - id: "headlines"
    name: "ğŸ”¥ ä»Šæ—¥å¤´æ¡"
    description: "..."
    enabled: true
    max_items: 5
    order: 1
    sources:
      - type: "rss" | "api" | "bilibili" | etc.
        name: "æ˜¾ç¤ºåç§°"
        url: "RSS URL"
        weight: 1.0
        filter:
          keywords: ["å…³é”®è¯"]
    organization:
      sort_by: "relevance" | "time" | "mixed"
      dedup_strategy: "semantic" | "exact" | "none"
      summarize: "3_points" | "1_sentence" | "paragraph" | "none"
```

è¯·ç”Ÿæˆé…ç½®:"""

        config_response = await self._chat_with_llm(generate_prompt)

        # æå– YAML ä»£ç å—
        yaml_content = self._extract_yaml(config_response)

        if not yaml_content:
            console.print("[red]âœ— æœªèƒ½ä» AI å“åº”ä¸­æå–æœ‰æ•ˆé…ç½®[/red]")
            console.print("AI å“åº”:")
            console.print(config_response)
            return False

        # æ˜¾ç¤ºé…ç½®
        console.print("\n[bold]ç”Ÿæˆçš„é…ç½®é¢„è§ˆ:[/bold]\n")
        console.print(Syntax(yaml_content, "yaml", theme="monokai"))

        # ä¿å­˜é…ç½®
        if Confirm.ask("\næ˜¯å¦ä¿å­˜æ­¤é…ç½®?", default=True):
            config_path = self.config_dir / "columns.yaml"

            # å¤‡ä»½æ—§é…ç½®
            if config_path.exists():
                backup_path = self.config_dir / "columns.yaml.backup"
                backup_path.write_text(config_path.read_text(), encoding="utf-8")
                console.print(f"[dim]å·²å¤‡ä»½åŸé…ç½®åˆ° {backup_path}[/dim]")

            # ä¿å­˜æ–°é…ç½®
            config_path.write_text(yaml_content, encoding="utf-8")
            console.print(f"[green]âœ“ é…ç½®å·²ä¿å­˜åˆ° {config_path}[/green]")

            # åŒæ—¶ä¿å­˜ .env
            await self._save_env()

            return True

        return False

    def _extract_yaml(self, text: str) -> Optional[str]:
        """ä»æ–‡æœ¬ä¸­æå– YAML ä»£ç å—"""
        import re

        # åŒ¹é… ```yaml ... ``` æˆ– ``` ... ```
        patterns = [
            r"```yaml\n(.*?)```",
            r"```yml\n(.*?)```",
            r"```\n(.*?)```",
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.DOTALL)
            if match:
                return match.group(1).strip()

        # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•æå–çœ‹èµ·æ¥åƒæ˜¯ YAML çš„éƒ¨åˆ†
        if "columns:" in text:
            start = text.find("columns:")
            return text[start:].strip()

        return None

    async def _save_env(self):
        """ä¿å­˜ LLM é…ç½®åˆ° .env"""
        env_path = Path(__file__).parent.parent / ".env"

        env_content = f"""
# LLM é…ç½®ï¼ˆç”±ä¸“å®¶æ¨¡å¼ç”Ÿæˆï¼‰
LLM_PROVIDER={self.llm_config.provider}
LLM_API_KEY={self.llm_config.api_key}
LLM_MODEL={self.llm_config.model}
"""

        if self.llm_config.base_url:
            env_content += f'LLM_BASE_URL={self.llm_config.base_url}\n'

        # å…¼å®¹æ—§é…ç½®
        if self.llm_config.provider == "openai":
            env_content += f"""
OPENAI_API_KEY={self.llm_config.api_key}
OPENAI_MODEL={self.llm_config.model}
"""
            if self.llm_config.base_url:
                env_content += f'OPENAI_BASE_URL={self.llm_config.base_url}\n'

        if env_path.exists():
            # è¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶
            with open(env_path, "a", encoding="utf-8") as f:
                f.write(env_content)
        else:
            env_path.write_text(env_content.strip(), encoding="utf-8")

        console.print(f"[green]âœ“ LLM é…ç½®å·²ä¿å­˜åˆ° {env_path}[/green]")


# CLI å…¥å£
async def run_expert_setup():
    """è¿è¡Œä¸“å®¶é…ç½®æ¨¡å¼"""
    setup = LLMAssistedSetup()
    return await setup.run()


if __name__ == "__main__":
    import asyncio
    asyncio.run(run_expert_setup())
